name: Scrape State Fish & Game Data

# Runs twice a month — scrapes ALL data from all 15 state F&G agencies:
#   - Draw odds & history         - Application deadlines
#   - License & tag fees          - Season dates
#   - Regulation changes          - Leftover tag availability
#   - Unit/GMU data               - Harvest statistics
#
# State data publication calendar:
#   Feb: AK draw results          May: NV, AZ, OR draw results
#   Jun: CO, WY, MT, ID, UT      Jul: NM draw results
#   Aug-Sep: Leftover/2nd draw    Year-round: fee/reg changes
#
# Twice monthly catches everything within ~2 weeks of publication
# while being respectful to state servers (2s delay between states).

on:
  schedule:
    # 1st of every month at 6:00 AM UTC (midnight MST)
    - cron: "0 6 1 * *"
    # 15th of every month at 6:00 AM UTC
    - cron: "0 6 15 * *"

  # Manual trigger from GitHub Actions UI
  workflow_dispatch:
    inputs:
      states:
        description: "State codes to scrape (space-separated, empty = all). Example: CO WY MT"
        required: false
        default: ""
        type: string

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 45

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm

      - run: npm ci

      - name: Run all state scrapers
        env:
          NEXT_PUBLIC_SUPABASE_URL: ${{ secrets.NEXT_PUBLIC_SUPABASE_URL }}
          SUPABASE_SERVICE_ROLE_KEY: ${{ secrets.SUPABASE_SERVICE_ROLE_KEY }}
        run: npx tsx scripts/scrapers/run-all.ts ${{ github.event.inputs.states }}

      - name: Upload scraper log
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-log-${{ github.run_id }}
          path: scripts/scrapers/*.log
          retention-days: 30
          if-no-files-found: ignore

      - name: Create issue on failure
        if: failure()
        uses: actions/github-script@v7
        with:
          script: |
            const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            const title = `Scraper failure — ${new Date().toISOString().split('T')[0]}`;
            const body = [
              `## Scraper Run Failed`,
              ``,
              `**Run:** [${context.runId}](${runUrl})`,
              `**Triggered by:** ${context.eventName}`,
              `**Time:** ${new Date().toISOString()}`,
              ``,
              `Check the [workflow logs](${runUrl}) and scraper artifacts for details.`,
              ``,
              `---`,
              `*Auto-created by scrape-draw-data workflow*`,
            ].join('\n');

            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title,
              body,
              labels: ['scraper', 'bug'],
            });
