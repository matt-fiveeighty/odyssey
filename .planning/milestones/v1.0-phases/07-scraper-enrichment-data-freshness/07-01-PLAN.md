---
phase: 07-scraper-enrichment-data-freshness
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/scrapers/base-scraper.ts
  - scripts/scrapers/schemas.ts
  - package.json
autonomous: true

must_haves:
  truths:
    - "BaseScraper exposes parseHtml() returning a CheerioAPI and extractTable() returning Record<string, string>[] for any HTML table selector"
    - "BaseScraper exposes fetchPdfBuffer() returning a Buffer and parsePdfText() returning extracted text, plus parsePdfTableLines() for heuristic table parsing"
    - "fetchPage() rejects responses larger than 5MB to prevent OOM in GitHub Actions"
    - "Plausibility-guarded Zod schemas reject fees outside $0-$10,000, dates outside 2024-2030, and success rates outside 0-100%"
    - "validateBatch() uses plausibility schemas and logs rejections without blocking valid data"
    - "cheerio and pdf-parse are installed as production dependencies"
  artifacts:
    - path: "scripts/scrapers/base-scraper.ts"
      provides: "Enhanced BaseScraper with HTML parsing (cheerio), PDF extraction (pdf-parse), and 5MB response guard"
      exports: ["BaseScraper", "ScrapedUnit", "ScrapedDrawHistory", "ScrapedDeadline", "ScrapedFee", "ScrapedSeason", "ScrapedRegulation", "ScrapedLeftoverTag"]
    - path: "scripts/scrapers/schemas.ts"
      provides: "Plausibility-guarded Zod schemas for all scraped data types"
      exports: ["PlausibleFeeSchema", "PlausibleDeadlineSchema", "PlausibleSeasonSchema", "PlausibleLeftoverTagSchema", "validateBatch"]
    - path: "package.json"
      provides: "cheerio and pdf-parse dependencies"
      contains: "cheerio"
  key_links:
    - from: "scripts/scrapers/base-scraper.ts"
      to: "cheerio"
      via: "import * as cheerio for parseHtml/extractTable"
      pattern: "import.*cheerio"
    - from: "scripts/scrapers/base-scraper.ts"
      to: "pdf-parse"
      via: "import pdfParse for parsePdfText"
      pattern: "import.*pdf-parse"
---

<objective>
Enhance BaseScraper with cheerio HTML parsing and pdf-parse PDF extraction capabilities, add a 5MB response size guard to fetchPage(), and harden Zod schemas with domain-specific plausibility refinements.

Purpose: This is the foundation that all downstream scraper implementations (OR, UT, remaining states) depend on. Without cheerio, scrapers must use fragile regex for HTML tables. Without pdf-parse, UT's PDF-based draw reports cannot be extracted. Without plausibility guards, bad scraped data (e.g., $0 fees, 1970 dates) flows silently into the UI.

Output: Enhanced `scripts/scrapers/base-scraper.ts` with three new parsing methods + response guard, and `scripts/scrapers/schemas.ts` with plausibility-refined schemas. cheerio and pdf-parse installed.
</objective>

<execution_context>
@/Users/mattramirez/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mattramirez/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@scripts/scrapers/base-scraper.ts
@scripts/scrapers/schemas.ts
@.planning/phases/07-scraper-enrichment-data-freshness/07-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install cheerio and pdf-parse, add HTML/PDF parsing methods to BaseScraper</name>
  <files>scripts/scrapers/base-scraper.ts, package.json</files>
  <action>
**Step 1: Install dependencies.**
Run `npm install cheerio pdf-parse`. cheerio 1.x has built-in TypeScript types. pdf-parse may need `@types/pdf-parse` -- check if types are included, install `@types/pdf-parse` as devDep if not.

**Step 2: Add imports to base-scraper.ts.**
At the top of the file, add:
```typescript
import * as cheerio from "cheerio";
import pdfParse from "pdf-parse";
```

**Step 3: Add 5MB response size guard to fetchPage().**
After the `res.ok` check in `fetchPage()`, add a Content-Length check:
```typescript
const contentLength = res.headers.get("content-length");
if (contentLength && parseInt(contentLength, 10) > 5 * 1024 * 1024) {
  throw new Error(`Response too large: ${contentLength} bytes (5MB limit)`);
}
```
Also, after `await res.text()`, add a guard on the actual text length:
```typescript
const text = await res.text();
if (text.length > 5 * 1024 * 1024) {
  throw new Error(`Response body too large: ${text.length} chars (5MB limit)`);
}
return text;
```

**Step 4: Add parseHtml() protected method.**
After the fetchPage() method, add:
```typescript
protected parseHtml(html: string): cheerio.CheerioAPI {
  return cheerio.load(html);
}
```

**Step 5: Add extractTable() protected method.**
Extracts rows from an HTML table by selector, using header names as keys. Match by header name (never by column index -- per research anti-patterns).
```typescript
protected extractTable(html: string, tableSelector: string): Record<string, string>[] {
  const $ = this.parseHtml(html);
  const rows: Record<string, string>[] = [];
  const headers: string[] = [];

  // Get headers from <th> elements
  $(`${tableSelector} th`).each((_, el) => {
    headers.push($(el).text().trim().toLowerCase());
  });

  // If no <th>, try first row of <td> as headers
  if (headers.length === 0) {
    $(`${tableSelector} tr:first-child td`).each((_, el) => {
      headers.push($(el).text().trim().toLowerCase());
    });
  }

  if (headers.length === 0) return rows;

  // Get data rows from <tbody tr> or all <tr> after first
  const dataRows = $(`${tableSelector} tbody tr`).length > 0
    ? $(`${tableSelector} tbody tr`)
    : $(`${tableSelector} tr`).slice(1);

  dataRows.each((_, tr) => {
    const row: Record<string, string> = {};
    $(tr).find("td").each((i, td) => {
      if (headers[i]) row[headers[i]] = $(td).text().trim();
    });
    if (Object.keys(row).length > 0) rows.push(row);
  });

  return rows;
}
```

**Step 6: Add fetchPdfBuffer() protected async method.**
Fetches a PDF as a binary Buffer (NOT as text -- this is the bug in the existing UT scraper).
```typescript
protected async fetchPdfBuffer(url: string, retries = 3): Promise<Buffer> {
  for (let attempt = 1; attempt <= retries; attempt++) {
    try {
      const res = await fetch(url, {
        headers: {
          "User-Agent": "HuntPlannerBot/1.0 (research; contact: admin@odysseyoutdoors.com)",
          Accept: "application/pdf",
        },
      });

      if (!res.ok) throw new Error(`HTTP ${res.status} ${res.statusText}`);

      const contentLength = res.headers.get("content-length");
      if (contentLength && parseInt(contentLength, 10) > 20 * 1024 * 1024) {
        throw new Error(`PDF too large: ${contentLength} bytes (20MB limit)`);
      }

      return Buffer.from(await res.arrayBuffer());
    } catch (err) {
      this.log(`fetchPdfBuffer attempt ${attempt}/${retries} failed: ${(err as Error).message}`);
      if (attempt === retries) throw err;
      await new Promise((r) => setTimeout(r, 1000 * Math.pow(2, attempt - 1)));
    }
  }
  throw new Error("fetchPdfBuffer exhausted retries");
}
```

**Step 7: Add parsePdfText() and parsePdfTableLines() protected methods.**
```typescript
protected async parsePdfText(buffer: Buffer): Promise<string> {
  const data = await pdfParse(buffer);
  return data.text;
}

/**
 * Heuristic table parser for PDF text: splits by newlines, then by 2+ whitespace.
 * Works for simple columnar layouts. Returns empty array if text is empty.
 */
protected parsePdfTableLines(text: string): string[][] {
  return text
    .split("\n")
    .filter((line) => line.trim().length > 0)
    .map((line) => line.split(/\s{2,}/).map((s) => s.trim()))
    .filter((cols) => cols.length >= 2);
}
```

**Important:** Do NOT modify any existing method signatures or the `run()` orchestrator. These are additive-only changes. Do NOT make `parseCsvText` public (it is already private and used by `fetchCsv`).
  </action>
  <verify>
Run `npx tsc --noEmit` to confirm no type errors. Verify cheerio and pdf-parse appear in package.json dependencies. Verify base-scraper.ts exports remain unchanged (BaseScraper + all interfaces).
  </verify>
  <done>
BaseScraper has 6 new methods: parseHtml() returning CheerioAPI, extractTable() returning keyed row arrays, fetchPdfBuffer() returning Buffer with retry+size guard, parsePdfText() returning extracted text, parsePdfTableLines() returning string[][] heuristic columns. fetchPage() rejects responses over 5MB. cheerio and pdf-parse are in package.json. All existing tests pass and no type errors.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add plausibility refinements to Zod schemas</name>
  <files>scripts/scrapers/schemas.ts</files>
  <action>
Enhance the existing Zod schemas in `scripts/scrapers/schemas.ts` with domain-specific plausibility refinements. Keep the original schemas intact (they remain the structural validators). Add new `Plausible*` schemas that extend them.

**Add these plausibility schemas after the existing schemas:**

```typescript
// ---------------------------------------------------------------------------
// Plausibility-guarded schemas (domain-specific validation)
// ---------------------------------------------------------------------------

/** Fee amounts must be > $0 and < $10,000. Tag fees of $0 are implausible. */
export const PlausibleFeeSchema = ScrapedFeeSchema.refine(
  (fee) => fee.amount > 0 && fee.amount < 10000,
  { message: "Fee amount outside plausible range ($0-$10,000)" }
).refine(
  (fee) => !(fee.amount === 0 && fee.feeName.toLowerCase().includes("tag")),
  { message: "Tag cost of $0 is implausible" }
);

/** Deadline dates must be between 2024 and 2030. */
export const PlausibleDeadlineSchema = ScrapedDeadlineSchema.refine(
  (dl) => {
    const parsed = new Date(dl.date);
    if (isNaN(parsed.getTime())) return false;
    const year = parsed.getFullYear();
    return year >= 2024 && year <= 2030;
  },
  { message: "Deadline date outside plausible range (2024-2030) or unparseable" }
);

/** Season dates must be parseable and year between 2024-2030. Start must be before end. */
export const PlausibleSeasonSchema = ScrapedSeasonSchema.refine(
  (s) => s.year >= 2024 && s.year <= 2030,
  { message: "Season year outside plausible range (2024-2030)" }
);

/** Leftover tags must have at least 1 tag available. */
export const PlausibleLeftoverTagSchema = ScrapedLeftoverTagSchema.refine(
  (lt) => lt.tagsAvailable >= 1,
  { message: "Leftover tags must have at least 1 available" }
);

/** Draw history: odds 0-100%, applicants and tags non-negative, year 2000-2050 */
export const PlausibleDrawHistorySchema = ScrapedDrawHistorySchema.refine(
  (dh) => dh.odds >= 0 && dh.odds <= 100,
  { message: "Draw odds outside 0-100% range" }
);
```

**Update the validateBatch() function** to accept an optional `plausibilitySchema` parameter:

The current `validateBatch<T>(rows, schema, label, log)` signature stays the same. The caller passes whichever schema they want -- base or plausible. No changes needed to the function signature, but add a JSDoc note:

```typescript
/**
 * Validate an array of scraped records against a schema.
 * Returns valid rows and logs invalid ones -- never blocks the pipeline.
 *
 * Pass a Plausible*Schema for domain-specific guards (recommended for DB upsert),
 * or the base schema for structural validation only.
 */
```

This keeps backward compatibility -- existing scrapers calling `validateBatch(rows, ScrapedFeeSchema, ...)` keep working. New scrapers use `validateBatch(rows, PlausibleFeeSchema, ...)` for stricter checks.
  </action>
  <verify>
Run `npx tsc --noEmit` to confirm no type errors. Verify the file exports PlausibleFeeSchema, PlausibleDeadlineSchema, PlausibleSeasonSchema, PlausibleLeftoverTagSchema, and PlausibleDrawHistorySchema. Verify existing exports (ScrapedUnitSchema, etc.) still exist.
  </verify>
  <done>
schemas.ts exports both structural schemas (ScrapedFeeSchema, etc.) and plausibility-guarded schemas (PlausibleFeeSchema, etc.). Plausible schemas reject: fees outside $0-$10k, $0 tag costs, dates outside 2024-2030, unparseable dates, season years outside range, and leftover tags with 0 available. validateBatch() works with either schema type. All existing imports remain functional.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors
2. `npm ls cheerio pdf-parse` shows both installed
3. base-scraper.ts has parseHtml, extractTable, fetchPdfBuffer, parsePdfText, parsePdfTableLines methods
4. base-scraper.ts fetchPage() has 5MB response guard
5. schemas.ts has 5 Plausible* schema exports alongside 7 original schema exports
6. No existing scraper files need modification (purely additive changes)
</verification>

<success_criteria>
- BaseScraper provides cheerio HTML parsing and pdf-parse PDF extraction as protected methods available to all 15 state scrapers
- fetchPage() has a 5MB size guard preventing OOM in GitHub Actions
- fetchPdfBuffer() fetches PDFs as binary Buffers (not text) with retry and 20MB size guard
- Plausibility-guarded Zod schemas exist for all data types that will be scraped in subsequent plans
- Zero breaking changes to existing scrapers or the run-all.ts orchestrator
</success_criteria>

<output>
After completion, create `.planning/phases/07-scraper-enrichment-data-freshness/07-01-SUMMARY.md`
</output>
