---
phase: 07-scraper-enrichment-data-freshness
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - scripts/scrapers/or-draw-data.ts
  - scripts/scrapers/ut-draw-data.ts
  - src/lib/scrapers/fingerprint.ts
  - src/lib/scrapers/plausibility.ts
autonomous: true

must_haves:
  truths:
    - "Oregon scraper uses cheerio extractTable() instead of regex for all HTML table parsing"
    - "Utah scraper uses fetchPdfBuffer() + parsePdfText() instead of fetchPage() for PDF draw reports"
    - "Both OR and UT scrapeDeadlines(), scrapeFees(), scrapeSeasons(), and scrapeLeftoverTags() use plausibility-guarded schemas via validateBatch()"
    - "Structural fingerprinting computes a SHA-256 hash of CSS selector paths for key page elements and compares to last-known fingerprint"
    - "When fingerprint changes, the scraper logs a warning and stores the new fingerprint but does NOT skip the scrape"
    - "Never-overwrite-good-data guard: if a scraper returns 0 rows but DB has existing data, the upsert is skipped with a warning"
  artifacts:
    - path: "scripts/scrapers/or-draw-data.ts"
      provides: "Oregon scraper upgraded to cheerio-based HTML parsing with plausibility validation"
      exports: ["OregonScraper"]
    - path: "scripts/scrapers/ut-draw-data.ts"
      provides: "Utah scraper upgraded to binary PDF extraction with plausibility validation"
      exports: ["UtahScraper"]
    - path: "src/lib/scrapers/fingerprint.ts"
      provides: "Structural fingerprinting: compute, compare, and store page structure hashes"
      exports: ["computeFingerprint", "compareFingerprint", "storeFingerprint", "StructuralFingerprint"]
    - path: "src/lib/scrapers/plausibility.ts"
      provides: "Never-overwrite-good-data guard and row count sanity check"
      exports: ["guardAgainstDataLoss", "checkRowCountSanity"]
  key_links:
    - from: "scripts/scrapers/or-draw-data.ts"
      to: "scripts/scrapers/base-scraper.ts"
      via: "Uses extractTable() and parseHtml() from enhanced BaseScraper"
      pattern: "this\\.extractTable|this\\.parseHtml"
    - from: "scripts/scrapers/ut-draw-data.ts"
      to: "scripts/scrapers/base-scraper.ts"
      via: "Uses fetchPdfBuffer() and parsePdfText() from enhanced BaseScraper"
      pattern: "this\\.fetchPdfBuffer|this\\.parsePdfText"
    - from: "src/lib/scrapers/fingerprint.ts"
      to: "cheerio"
      via: "Uses cheerio.load() for DOM structure analysis"
      pattern: "cheerio\\.load"
    - from: "src/lib/scrapers/plausibility.ts"
      to: "@supabase/supabase-js"
      via: "Checks existing row counts before allowing upsert"
      pattern: "supabase.*count"
---

<objective>
Upgrade the Oregon (CSV/HTML) and Utah (REST/PDF) scrapers to use the enhanced BaseScraper parsing methods, add structural fingerprinting for change detection, and implement the never-overwrite-good-data guard.

Purpose: OR and UT are the proof-of-concept scrapers that validate the cheerio and pdf-parse integrations built in 07-01. Their success pattern becomes the template for all remaining states. Fingerprinting detects when state websites change layout (preventing silent misparsing), and the data-loss guard prevents transient failures from wiping good data.

Output: Upgraded `or-draw-data.ts` and `ut-draw-data.ts` with cheerio/pdf-parse parsing and plausibility validation. New `src/lib/scrapers/fingerprint.ts` for structural change detection and `src/lib/scrapers/plausibility.ts` for data-loss prevention.
</objective>

<execution_context>
@/Users/mattramirez/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mattramirez/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-scraper-enrichment-data-freshness/07-01-SUMMARY.md

@scripts/scrapers/base-scraper.ts
@scripts/scrapers/or-draw-data.ts
@scripts/scrapers/ut-draw-data.ts
@scripts/scrapers/schemas.ts
@.planning/phases/07-scraper-enrichment-data-freshness/07-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Structural fingerprinting module and never-overwrite-good-data guard</name>
  <files>src/lib/scrapers/fingerprint.ts, src/lib/scrapers/plausibility.ts</files>
  <action>
**Create `src/lib/scrapers/fingerprint.ts`:**

This module computes a structural fingerprint of a web page by hashing the CSS selector paths of key elements (tables, forms, data containers). It stores fingerprints in Supabase and compares against the last-known fingerprint to detect website redesigns.

```typescript
import * as cheerio from "cheerio";
import { createHash } from "crypto";

export interface StructuralFingerprint {
  stateId: string;
  url: string;
  selectorHash: string;         // SHA-256 of sorted selector paths (first 16 chars)
  selectorPaths: string[];      // e.g. ["body > div.content > table.draw-data"]
  scrapedAt: string;            // ISO timestamp
}
```

**computeFingerprint(html: string, url: string, stateId: string): StructuralFingerprint**
1. Load HTML with `cheerio.load(html)`.
2. Collect structural skeleton: iterate all `table`, `form`, `main`, `article`, `section` elements.
3. For each element, build a path: walk up parents collecting `tagName.className` at each level (max depth 5). Sort class names alphabetically. Format: `depth:tag.class1.class2`.
4. Sort all paths alphabetically.
5. Hash the joined paths with SHA-256, take first 16 hex chars as `selectorHash`.
6. Return the StructuralFingerprint object.

**compareFingerprint(current: StructuralFingerprint, previous: StructuralFingerprint | null): { changed: boolean; details: string }**
1. If no previous fingerprint, return `{ changed: false, details: "First fingerprint recorded" }`.
2. If `current.selectorHash === previous.selectorHash`, return `{ changed: false, details: "Structure unchanged" }`.
3. Otherwise, compare `selectorPaths` arrays: find added, removed, and common paths. Return `{ changed: true, details: "Structure changed: {N} paths added, {M} removed" }`.

**storeFingerprint(fingerprint: StructuralFingerprint, supabase: SupabaseClient): Promise<void>**
1. Upsert into `scraper_fingerprints` table (create if needed -- but for now, just write the upsert code assuming the table exists with columns: `state_id`, `url`, `selector_hash`, `selector_paths`, `scraped_at`). Use conflict on `(state_id, url)`.
2. Log the upsert result.

**getLastFingerprint(stateId: string, url: string, supabase: SupabaseClient): Promise<StructuralFingerprint | null>**
1. Query `scraper_fingerprints` for matching `state_id` and `url`.
2. Return mapped StructuralFingerprint or null.

Import SupabaseClient type from `@supabase/supabase-js`.

**Create `src/lib/scrapers/plausibility.ts`:**

This module provides the "never overwrite good data" guard.

**guardAgainstDataLoss(table: string, stateId: string, newRowCount: number, supabase: SupabaseClient, log: (msg: string) => void): Promise<{ safe: boolean; reason: string }>**
1. If `newRowCount > 0`, return `{ safe: true, reason: "Has data" }`.
2. Query `supabase.from(table).select("*", { count: "exact", head: true }).eq("state_id", stateId)`.
3. If existing count is 0, return `{ safe: true, reason: "No existing data to protect" }`.
4. If existing count > 0 and newRowCount === 0, return `{ safe: false, reason: "Scraper returned 0 rows but DB has ${count} existing rows. Skipping upsert to protect data." }`.
5. Log the result.

**checkRowCountSanity(table: string, stateId: string, newCount: number, supabase: SupabaseClient, log: (msg: string) => void): Promise<{ plausible: boolean; dropPercent: number }>**
1. Query existing count from `table` where `state_id = stateId`.
2. If no existing data (count 0), return `{ plausible: true, dropPercent: 0 }`.
3. Compute `dropPercent = ((existingCount - newCount) / existingCount) * 100`.
4. If `dropPercent > 80`, return `{ plausible: false, dropPercent }` and log warning about >80% row count drop (likely scraper failure, not real data change).
5. Otherwise return `{ plausible: true, dropPercent }`.

Both functions import `SupabaseClient` from `@supabase/supabase-js`. Both are pure utility functions, not tied to BaseScraper.
  </action>
  <verify>
Run `npx tsc --noEmit` to confirm no type errors. Verify both files exist and export their functions.
  </verify>
  <done>
fingerprint.ts computes SHA-256 structural fingerprints from HTML DOM structure, compares against stored fingerprints, and persists to Supabase. plausibility.ts prevents data loss by checking existing row counts before allowing empty upserts and flags >80% row count drops as likely scraper failures. Both modules are pure utilities callable from any scraper.
  </done>
</task>

<task type="auto">
  <name>Task 2: Upgrade Oregon and Utah scrapers to use cheerio, pdf-parse, fingerprinting, and plausibility validation</name>
  <files>scripts/scrapers/or-draw-data.ts, scripts/scrapers/ut-draw-data.ts</files>
  <action>
**Oregon scraper upgrade (`scripts/scrapers/or-draw-data.ts`):**

1. **Replace all regex HTML parsing with cheerio.** The current `parseHtmlTables()`, `extractDownloadLinks()`, and `extractCsvLinks()` methods use regex patterns like `/<table[\s\S]*?<\/table>/gi`. Replace each with cheerio-based equivalents:

   - `extractDownloadLinks(html)`: Use `this.parseHtml(html)` to get `$`. Find all `a[href]` elements where href matches `.csv`, `.xlsx`, `.xls`, `.pdf`. Extract href and label text. Resolve relative URLs against the base URL.

   - `extractCsvLinks(html)`: Use `$` to find all `a[href$=".csv"]`. Resolve relative URLs.

   - `parseHtmlTables(html)`: Replace the entire method body with `this.extractTable(html, "table")` call, then map results through `parseOrRow()`. If multiple tables exist, iterate them with `$("table").each()` and extract from each.

2. **Add fingerprinting to scrapeUnits() and scrapeDrawHistory().** At the start of each page fetch, after getting HTML, compute a fingerprint:
   ```typescript
   import { computeFingerprint, compareFingerprint, storeFingerprint, getLastFingerprint } from "@/lib/scrapers/fingerprint";

   // Inside scrapeUnits() after fetching ODFW_CONTROLLED_HUNTS:
   const fp = computeFingerprint(html, ODFW_CONTROLLED_HUNTS, "OR");
   const lastFp = await getLastFingerprint("OR", ODFW_CONTROLLED_HUNTS, this.supabase);
   const fpResult = compareFingerprint(fp, lastFp);
   if (fpResult.changed) {
     this.log(`  âš  Structure change detected: ${fpResult.details}`);
   }
   await storeFingerprint(fp, this.supabase);
   ```

3. **Add plausibility validation to scrapeDeadlines(), scrapeFees(), scrapeSeasons(), scrapeLeftoverTags().** Before returning results from each method, validate through plausibility schemas:
   ```typescript
   import { validateBatch } from "./schemas";
   import { PlausibleDeadlineSchema, PlausibleFeeSchema, PlausibleSeasonSchema, PlausibleLeftoverTagSchema } from "./schemas";

   // At end of scrapeDeadlines():
   return validateBatch(deadlines, PlausibleDeadlineSchema, "OR deadlines", this.log.bind(this));
   ```
   Apply same pattern for fees (PlausibleFeeSchema), seasons (PlausibleSeasonSchema), leftover tags (PlausibleLeftoverTagSchema).

4. **Remove all regex HTML patterns.** After migration, there should be zero instances of `/<table/`, `/<th/`, `/<td/`, `/<tr/`, `/<a[^>]*href/` regex patterns. All HTML parsing goes through cheerio.

**Utah scraper upgrade (`scripts/scrapers/ut-draw-data.ts`):**

1. **Fix PDF fetching.** The current `scrapeDrawHistory()` calls `this.fetchPage(report.url)` for PDF URLs -- this fetches the PDF as text (garbled binary). Replace with:
   ```typescript
   const buffer = await this.fetchPdfBuffer(report.url);
   const text = await this.parsePdfText(buffer);
   ```
   Update the `parsePdfText()` private method to accept a `string` (already extracted text) instead of fetching. Actually, rename the existing private `parsePdfText(text, speciesIds)` to `parsePdfDrawData(text, speciesIds)` to avoid name collision with the inherited `parsePdfText(buffer)` from BaseScraper.

2. **Replace regex HTML table parsing.** Same as Oregon -- replace `parseHtmlTables()` with cheerio `extractTable()` calls.

3. **Add fingerprinting to scrapeUnits() and scrapeDrawHistory().** Same pattern as Oregon, fingerprinting the DWR_ODDS_PAGE and DWR_DRAW_ODDS_TOOL pages.

4. **Add plausibility validation.** Same pattern as Oregon -- validate through Plausible schemas before returning from each scrape method.

5. **Remove all regex HTML patterns.** Same as Oregon -- zero regex HTML parsing after migration.

**Important for both scrapers:**
- Keep all existing scraping logic (URL lists, species mapping, CSV handling) -- only replace the HTML parsing mechanism.
- The structured fee data (hardcoded fee tables in scrapeFees()) stays as-is -- it is intentionally hardcoded as the primary source of truth.
- Import paths for fingerprint.ts and plausibility.ts use `@/lib/scrapers/...` syntax (these run in the scripts context, so verify tsconfig path resolution; if `@/` doesn't resolve in scripts, use relative paths like `../../src/lib/scrapers/...`).
  </action>
  <verify>
1. Run `npx tsc --noEmit` to confirm no type errors.
2. Search for regex HTML patterns in both files: `grep -E '/<(table|tr|th|td|a)' scripts/scrapers/or-draw-data.ts scripts/scrapers/ut-draw-data.ts` should return 0 matches.
3. Verify OR scraper imports from cheerio via BaseScraper methods (extractTable, parseHtml).
4. Verify UT scraper uses fetchPdfBuffer + parsePdfText instead of fetchPage for PDF URLs.
5. Verify both scrapers import and use PlausibleFeeSchema etc. from schemas.ts.
  </verify>
  <done>
Oregon scraper uses cheerio extractTable() for all HTML parsing (zero regex patterns remain). Utah scraper uses fetchPdfBuffer() + parsePdfText() for PDF draw reports (no more garbled text-mode PDF fetching). Both scrapers compute structural fingerprints on key pages and validate outputs through plausibility-guarded schemas. Both maintain all existing scraping logic (URLs, species mapping, structured fees).
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no errors
2. Zero regex HTML patterns (`/<table|/<tr|/<th|/<td`) in or-draw-data.ts and ut-draw-data.ts
3. UT scraper calls `this.fetchPdfBuffer()` (not `this.fetchPage()`) for all PDF URLs
4. Both scrapers import from `./schemas` for plausibility validation
5. fingerprint.ts exports computeFingerprint, compareFingerprint, storeFingerprint, getLastFingerprint
6. plausibility.ts exports guardAgainstDataLoss, checkRowCountSanity
7. `npx tsx scripts/scrapers/run-all.ts OR` and `npx tsx scripts/scrapers/run-all.ts UT` execute without crash (data results depend on live website availability)
</verification>

<success_criteria>
- Oregon scraper is fully cheerio-based with zero regex HTML parsing
- Utah scraper fetches PDFs as binary buffers and extracts text via pdf-parse
- Both scrapers validate outputs through plausibility-guarded Zod schemas
- Structural fingerprinting detects page structure changes without blocking scrapes
- Never-overwrite-good-data guard prevents empty scrapes from wiping existing data
- Both scrapers remain fully functional with the run-all.ts orchestrator
</success_criteria>

<output>
After completion, create `.planning/phases/07-scraper-enrichment-data-freshness/07-02-SUMMARY.md`
</output>
